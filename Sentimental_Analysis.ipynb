{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":88330,"databundleVersionId":10113261,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n#You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:34:03.056116Z","iopub.execute_input":"2024-11-11T15:34:03.057313Z","iopub.status.idle":"2024-11-11T15:34:04.471304Z","shell.execute_reply.started":"2024-11-11T15:34:03.057246Z","shell.execute_reply":"2024-11-11T15:34:04.469949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport random\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:34:15.663534Z","iopub.execute_input":"2024-11-11T15:34:15.664808Z","iopub.status.idle":"2024-11-11T15:34:15.669950Z","shell.execute_reply.started":"2024-11-11T15:34:15.664750Z","shell.execute_reply":"2024-11-11T15:34:15.668669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/ml-hackathon-ec-campus-set-3-alt/train/text.csv', encoding='ISO-8859-1')\n# Define path to video clips\nvideo_dir = '/kaggle/input/ml-hackathon-ec-campus-set-3-alt/train/videos'\n\n\n# Function to get video file path from IDs\ndef get_video_clip_path(row):\n    dialogue_id = row['Dialogue_ID']\n    utterance_id = row['Utterance_ID']\n    filename = f\"dia{dialogue_id}_utt{utterance_id}.mp4\"\n    return os.path.join(video_dir, filename)\n\n# Apply the function to get file paths for each sampled clip\ndata['video_clip_path'] = data.apply(get_video_clip_path, axis=1)\n\n# Check sample paths\nprint(data[['Dialogue_ID', 'Utterance_ID', 'video_clip_path']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:34:56.526442Z","iopub.execute_input":"2024-11-11T15:34:56.527697Z","iopub.status.idle":"2024-11-11T15:34:56.594012Z","shell.execute_reply.started":"2024-11-11T15:34:56.527617Z","shell.execute_reply":"2024-11-11T15:34:56.592860Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:35:13.512644Z","iopub.execute_input":"2024-11-11T15:35:13.513086Z","iopub.status.idle":"2024-11-11T15:35:13.522161Z","shell.execute_reply.started":"2024-11-11T15:35:13.513043Z","shell.execute_reply":"2024-11-11T15:35:13.520725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define path to video clips\ndf = pd.read_csv('/kaggle/input/ml-hackathon-ec-campus-set-3-alt/test/text.csv', encoding='ISO-8859-1')\nvideo_dir = '/kaggle/input/ml-hackathon-ec-campus-set-3-alt/test/videos'\n\n\n# Function to get video file path from IDs\ndef get_video_clip_path(row):\n    dialogue_id = row['Dialogue_ID']\n    utterance_id = row['Utterance_ID']\n    filename = f\"dia{dialogue_id}_utt{utterance_id}.mp4\"\n    return os.path.join(video_dir, filename)\n\n# Apply the function to get file paths for each sampled clip\ndf['video_clip_path'] = df.apply(get_video_clip_path, axis=1)\n\n# Check sample paths\nprint(df[['Dialogue_ID', 'Utterance_ID', 'video_clip_path']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:36:08.159873Z","iopub.execute_input":"2024-11-11T15:36:08.160277Z","iopub.status.idle":"2024-11-11T15:36:08.182065Z","shell.execute_reply.started":"2024-11-11T15:36:08.160239Z","shell.execute_reply":"2024-11-11T15:36:08.180854Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:36:50.941361Z","iopub.execute_input":"2024-11-11T15:36:50.941840Z","iopub.status.idle":"2024-11-11T15:36:50.963853Z","shell.execute_reply.started":"2024-11-11T15:36:50.941795Z","shell.execute_reply":"2024-11-11T15:36:50.962662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking for missing values in the dataset\nmissing_values = data.isnull().sum()\n\n# Basic text cleaning on 'Utterance' column: removing any stray characters or encoding issues\ndata['Utterance'] = data['Utterance'].str.replace(r'[^\\x00-\\x7F]+', '', regex=True)\n\n# Checking the unique values in categorical columns for consistency\nunique_values = {\n    \"Speaker\": data['Speaker'].unique(),\n    \"Sentiment\": data['Sentiment'].unique()\n}\n\nmissing_values, unique_values\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:37:09.896884Z","iopub.execute_input":"2024-11-11T15:37:09.898108Z","iopub.status.idle":"2024-11-11T15:37:09.921214Z","shell.execute_reply.started":"2024-11-11T15:37:09.898054Z","shell.execute_reply":"2024-11-11T15:37:09.920053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from textblob import TextBlob\n\n# Text-based feature extraction on 'Utterance' column\ndata['word_count'] = data['Utterance'].apply(lambda x: len(x.split()))\ndata['char_count'] = data['Utterance'].apply(len)\n\n# Sentiment polarity as a feature (using TextBlob)\ndata['text_sentiment'] = data['Utterance'].apply(lambda x: TextBlob(x).sentiment.polarity)\n\n# Display the first few rows with new features\ndata[['Utterance', 'word_count', 'char_count', 'text_sentiment']].head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:37:25.537258Z","iopub.execute_input":"2024-11-11T15:37:25.538252Z","iopub.status.idle":"2024-11-11T15:37:27.155665Z","shell.execute_reply.started":"2024-11-11T15:37:25.538202Z","shell.execute_reply":"2024-11-11T15:37:27.154486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datetime import datetime\n\n# Convert StartTime and EndTime to duration in seconds\ndef convert_to_seconds(time_str):\n    # Convert \"HH:MM:SS,ms\" to total seconds\n    try:\n        time_obj = datetime.strptime(time_str, '%H:%M:%S,%f')\n        return time_obj.hour * 3600 + time_obj.minute * 60 + time_obj.second + time_obj.microsecond / 1e6\n    except ValueError:\n        # Handle cases with shorter time format, e.g., \"H:MM:SS,ms\"\n        time_obj = datetime.strptime(time_str, '%H:%M:%S,%f')\n        return time_obj.hour * 3600 + time_obj.minute * 60 + time_obj.second + time_obj.microsecond / 1e6\n\n# Applying the conversion to StartTime and EndTime columns\ndata['StartTime_sec'] = data['StartTime'].apply(convert_to_seconds)\ndata['EndTime_sec'] = data['EndTime'].apply(convert_to_seconds)\n\n# Calculating the duration of each utterance\ndata['duration_sec'] = data['EndTime_sec'] - data['StartTime_sec']\n\n# Display the first few rows with the new duration feature\ndata[['StartTime', 'EndTime', 'StartTime_sec', 'EndTime_sec', 'duration_sec']].head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:37:42.112387Z","iopub.execute_input":"2024-11-11T15:37:42.113542Z","iopub.status.idle":"2024-11-11T15:37:42.176396Z","shell.execute_reply.started":"2024-11-11T15:37:42.113493Z","shell.execute_reply":"2024-11-11T15:37:42.175177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from gensim.models import Word2Vec\nimport numpy as np\n\n# Tokenize the Utterances for Word2Vec (splitting each utterance into words)\ndata['tokenized_utterance'] = data['Utterance'].apply(lambda x: x.lower().split())\n\n# Train a Word2Vec model on the tokenized utterances\nword2vec_model = Word2Vec(sentences=data['tokenized_utterance'], vector_size=100, window=5, min_count=1, sg=1, epochs=10)\n\n# Function to get the average Word2Vec embedding for an utterance\ndef get_avg_word2vec(tokens, model, vector_size=100):\n    # Filter words that are in the model's vocabulary\n    valid_words = [token for token in tokens if token in model.wv.key_to_index]\n    # If there are valid words, calculate the mean; otherwise, return a zero vector\n    if valid_words:\n        return np.mean(model.wv[valid_words], axis=0)\n    else:\n        return np.zeros(vector_size)\n\n# Apply the function to each tokenized utterance\ndata['w2v_embedding'] = data['tokenized_utterance'].apply(lambda x: get_avg_word2vec(x, word2vec_model))\n\n# Display the first few Word2Vec embeddings\ndata[['Utterance', 'w2v_embedding']].head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:38:06.196266Z","iopub.execute_input":"2024-11-11T15:38:06.197258Z","iopub.status.idle":"2024-11-11T15:38:22.572444Z","shell.execute_reply.started":"2024-11-11T15:38:06.197212Z","shell.execute_reply":"2024-11-11T15:38:22.571188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:39:12.098866Z","iopub.execute_input":"2024-11-11T15:39:12.099509Z","iopub.status.idle":"2024-11-11T15:39:25.517522Z","shell.execute_reply.started":"2024-11-11T15:39:12.099465Z","shell.execute_reply":"2024-11-11T15:39:25.516263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport os\n\n# Define the directory containing the videos and the output directory for frames\nvideo_dir = '/kaggle/input/ml-hackathon-ec-campus-set-3-alt/train/videos'  # Update this path\noutput_dir = '/kaggle/working/extracted_frames'\nos.makedirs(output_dir, exist_ok=True)\n\n# Parameters\nframe_interval = 10  # Extract every 10th frame\nbatch_size = 50      # Process 50 videos per batch for memory efficiency\n\n# Initialize a list to keep track of paths of saved frames\nextracted_frames = {}\n\n# Function to process a batch of videos\ndef process_video_batch(video_files):\n    for video_path in video_files:\n        video_name = os.path.basename(video_path).split('.')[0]  # File name without extension\n        cap = cv2.VideoCapture(video_path)\n        frame_count = 0\n        frame_list = []\n        \n        while cap.isOpened():\n            ret, frame = cap.read()\n            if not ret:\n                break\n            if frame_count % frame_interval == 0:\n                # Resize and save frame\n                frame_resized = cv2.resize(frame, (224, 224))\n                frame_filename = f\"{video_name}_frame{frame_count}.jpg\"\n                frame_path = os.path.join(output_dir, frame_filename)\n                cv2.imwrite(frame_path, frame_resized)\n                frame_list.append(frame_path)\n                \n            frame_count += 1\n        \n        cap.release()\n        extracted_frames[video_name] = frame_list\n\n# Main loop to process all videos in batches\nall_video_files = [os.path.join(video_dir, f) for f in os.listdir(video_dir) if f.endswith('.mp4')]\nfor i in range(0, len(all_video_files), batch_size):\n    batch_files = all_video_files[i:i + batch_size]\n    process_video_batch(batch_files)\n\n# Display paths to verify\nprint(\"Frames have been extracted and saved. Example paths:\")\nfor video, frames in list(extracted_frames.items())[:5]:  # Display only the first few entries for brevity\n    print(f\"Video: {video}, Frames: {frames[:3]}\")  # Show first 3 frames for each video\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:39:49.066193Z","iopub.execute_input":"2024-11-11T15:39:49.067000Z","iopub.status.idle":"2024-11-11T15:43:41.900628Z","shell.execute_reply.started":"2024-11-11T15:39:49.066952Z","shell.execute_reply":"2024-11-11T15:43:41.899455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport os\nimport pandas as pd\nimport time\nfrom scipy.ndimage import convolve\nfrom skimage.color import rgb2gray\n\n# Define directory paths\nvideo_dir = '/kaggle/input/ml-hackathon-ec-campus-set-3-alt/train/videos'  \noutput_file = '/kaggle/working/video_features.csv'\nframe_interval = 10\nbatch_size = 50  \n\n# Define a simple CNN with two convolutional layers and pooling\ndef simple_cnn_feature_extractor(image):\n    image = rgb2gray(image)  # Convert to grayscale\n    conv1_kernel = np.array([[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]])\n    conv1_output = convolve(image, conv1_kernel[0], mode='reflect')\n    pooled1 = conv1_output[::2, ::2]  # 2x2 pooling\n\n    conv2_kernel = np.array([[[1, 0, -1], [0, 0, 0], [-1, 0, 1]]])\n    conv2_output = convolve(pooled1, conv2_kernel[0], mode='reflect')\n    pooled2 = conv2_output[::2, ::2]  # 2x2 pooling again\n\n    return pooled2.flatten()  # Flatten for feature vector\n\n# Extract features for each video and average features across frames\ndef extract_video_features_with_custom_cnn(video_file):\n    cap = cv2.VideoCapture(video_file)\n    frame_features = []\n    frame_count = 0\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if frame_count % frame_interval == 0:\n            frame_resized = cv2.resize(frame, (64, 64))\n            features = simple_cnn_feature_extractor(frame_resized)\n            frame_features.append(features)\n        frame_count += 1\n\n    cap.release()\n    return np.mean(frame_features, axis=0) if frame_features else np.zeros(256)\n\n# Function to process videos in batches and save features incrementally\ndef process_video_batch(video_files):\n    batch_start_time = time.time()  # Start timing the batch\n    video_features_list = []\n    \n    for video_path in video_files:\n        video_name = os.path.basename(video_path).split('.')[0]\n        features = extract_video_features_with_custom_cnn(video_path)\n        video_features_list.append((video_name, features))\n\n    # Convert to DataFrame for easy saving\n    batch_df = pd.DataFrame(video_features_list, columns=['video_file', 'features'])\n    batch_df['features'] = batch_df['features'].apply(lambda x: x.tolist())  # Convert numpy array to list\n    batch_df.to_csv(output_file, mode='a', index=False, header=not os.path.exists(output_file))\n    \n    batch_end_time = time.time()  # End timing the batch\n    print(f\"Processed batch of {len(video_files)} videos in {batch_end_time - batch_start_time:.2f} seconds.\")\n\n# Main loop to process all videos in batches\ntotal_start_time = time.time()  # Start timing the entire process\nall_video_files = [os.path.join(video_dir, f) for f in os.listdir(video_dir) if f.endswith('.mp4')]\n\nfor i in range(0, len(all_video_files), batch_size):\n    batch_files = all_video_files[i:i + batch_size]\n    process_video_batch(batch_files)\n    print(f\"Completed batch {i // batch_size + 1} of {len(all_video_files) // batch_size + 1}\")\n\ntotal_end_time = time.time()  # End timing the entire process\nprint(f\"Total processing time for all videos: {total_end_time - total_start_time:.2f} seconds.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:44:53.115515Z","iopub.execute_input":"2024-11-11T15:44:53.115976Z","iopub.status.idle":"2024-11-11T15:48:42.691478Z","shell.execute_reply.started":"2024-11-11T15:44:53.115932Z","shell.execute_reply":"2024-11-11T15:48:42.690356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Load video features\nvideo_features_df = pd.read_csv('/kaggle/working/video_features.csv', converters={'features': eval})\n\n# Ensure the feature vectors are numpy arrays\nvideo_features_df['features'] = video_features_df['features'].apply(lambda x: np.array(x))\n\n# Display the dataframe structure to confirm loading\nprint(\"Video features loaded. Structure:\")\nprint(video_features_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:49:24.117753Z","iopub.execute_input":"2024-11-11T15:49:24.118605Z","iopub.status.idle":"2024-11-11T15:49:25.076674Z","shell.execute_reply.started":"2024-11-11T15:49:24.118545Z","shell.execute_reply":"2024-11-11T15:49:25.075398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Available columns in `data`:\", data.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:49:31.652344Z","iopub.execute_input":"2024-11-11T15:49:31.653352Z","iopub.status.idle":"2024-11-11T15:49:31.659596Z","shell.execute_reply.started":"2024-11-11T15:49:31.653305Z","shell.execute_reply":"2024-11-11T15:49:31.658257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Select relevant text features and embeddings from `data`\ntext_features_df = data[['video_clip_path', 'word_count', 'char_count', 'text_sentiment', 'duration_sec', 'w2v_embedding']].copy()\n\n# Combine structural features into a list for easier concatenation later\ntext_features_df['structural_features'] = text_features_df[['word_count', 'char_count', 'text_sentiment', 'duration_sec']].values.tolist()\n\n# Ensure Word2Vec embeddings are in array format (if not already)\ntext_features_df['w2v_embedding'] = text_features_df['w2v_embedding'].apply(lambda x: np.array(x) if not isinstance(x, np.ndarray) else x)\n\n# Display the first few rows of the text features DataFrame\nprint(\"Text features DataFrame:\")\nprint(text_features_df[['video_clip_path', 'structural_features', 'w2v_embedding']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:50:32.267186Z","iopub.execute_input":"2024-11-11T15:50:32.267641Z","iopub.status.idle":"2024-11-11T15:50:32.293754Z","shell.execute_reply.started":"2024-11-11T15:50:32.267598Z","shell.execute_reply":"2024-11-11T15:50:32.292311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Update `text_features_df` to use `video_file` for consistency\ntext_features_df['video_file'] = text_features_df['video_clip_path'].apply(lambda x: x.split('/')[-1].split('.')[0])\n\n# Merge `text_features_df` and `video_features_df` on `video_file`\ncombined_features_df = pd.merge(text_features_df, video_features_df, on='video_file', how='inner')\n\n# Verify the structure and contents of the merged DataFrame\nprint(\"combined_features_df structure after merging:\")\nprint(combined_features_df.info())\nprint(combined_features_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:54:57.509795Z","iopub.execute_input":"2024-11-11T15:54:57.510265Z","iopub.status.idle":"2024-11-11T15:54:57.562464Z","shell.execute_reply.started":"2024-11-11T15:54:57.510223Z","shell.execute_reply":"2024-11-11T15:54:57.561140Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check `video_features_df` structure and contents\nprint(\"video_features_df structure:\")\nprint(video_features_df.info())\nprint(video_features_df.head())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:55:33.237870Z","iopub.execute_input":"2024-11-11T15:55:33.238898Z","iopub.status.idle":"2024-11-11T15:55:33.257924Z","shell.execute_reply.started":"2024-11-11T15:55:33.238849Z","shell.execute_reply":"2024-11-11T15:55:33.256581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Standardize `video_file` in `text_features_df` by extracting the base file name\ntext_features_df['video_file'] = text_features_df['video_file'].apply(lambda x: x.split('/')[-1].split('.')[0])\n\n# Verify that the `video_file` columns now match in both DataFrames\nprint(\"Sample video_file values in text_features_df:\", text_features_df['video_file'].head())\nprint(\"Sample video_file values in video_features_df:\", video_features_df['video_file'].head())\n\n# Merge `text_features_df` and `video_features_df` on 'video_file'\ncombined_features_df = pd.merge(text_features_df, video_features_df, on='video_file', how='inner')\n\n# Verify the structure and contents of the merged DataFrame\nprint(\"combined_features_df structure after merging:\")\nprint(combined_features_df.info())\nprint(combined_features_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:55:43.702176Z","iopub.execute_input":"2024-11-11T15:55:43.702704Z","iopub.status.idle":"2024-11-11T15:55:43.741718Z","shell.execute_reply.started":"2024-11-11T15:55:43.702655Z","shell.execute_reply":"2024-11-11T15:55:43.740409Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Concatenate all features into a single vector, ensuring the result is a 1D array\ncombined_features_df['combined_features'] = combined_features_df.apply(\n    lambda row: np.concatenate([\n        np.array(row['structural_features']), \n        np.array(row['w2v_embedding']), \n        np.array(row['features'])\n    ]).ravel(), \n    axis=1\n)\n\n# Verify the structure of `combined_features`\nprint(\"Combined features DataFrame structure:\")\nprint(combined_features_df[['video_file', 'combined_features']].head())\nprint(\"Shape of each combined feature vector:\", combined_features_df['combined_features'].iloc[0].shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:55:59.458948Z","iopub.execute_input":"2024-11-11T15:55:59.460020Z","iopub.status.idle":"2024-11-11T15:55:59.514081Z","shell.execute_reply.started":"2024-11-11T15:55:59.459970Z","shell.execute_reply":"2024-11-11T15:55:59.512923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming `text_sentiment` is a polarity score, categorize it\ndef categorize_sentiment(polarity):\n    if polarity > 0.1:\n        return 'positive'\n    elif polarity < -0.1:\n        return 'negative'\n    else:\n        return 'neutral'\n\ncombined_features_df['sentiment_label'] = combined_features_df['text_sentiment'].apply(categorize_sentiment)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:56:26.362135Z","iopub.execute_input":"2024-11-11T15:56:26.363002Z","iopub.status.idle":"2024-11-11T15:56:26.369492Z","shell.execute_reply.started":"2024-11-11T15:56:26.362960Z","shell.execute_reply":"2024-11-11T15:56:26.368318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport numpy as np\n\n# Prepare features (X) and target labels (y) using the new categorical sentiment label\nX = np.stack(combined_features_df['combined_features'].values)\ny = combined_features_df['sentiment_label']  # Categorical labels\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train a Random Forest model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Model Accuracy:\", accuracy)\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:56:43.400214Z","iopub.execute_input":"2024-11-11T15:56:43.400636Z","iopub.status.idle":"2024-11-11T15:56:44.743113Z","shell.execute_reply.started":"2024-11-11T15:56:43.400590Z","shell.execute_reply":"2024-11-11T15:56:44.741841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming your DataFrame is called 'combined_features_df'\nprint(combined_features_df.columns)\nprint(combined_features_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:35:22.245771Z","iopub.execute_input":"2024-11-11T16:35:22.246227Z","iopub.status.idle":"2024-11-11T16:35:22.281444Z","shell.execute_reply.started":"2024-11-11T16:35:22.246185Z","shell.execute_reply":"2024-11-11T16:35:22.279746Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extracting the sentiment labels and Sr No. from the test data\nall_ids = test_df[\"Sr No.\"]\nsentiments = combined_features_df['sentiment_label']  # Use the 'sentiment_label' column\n\n# Create the DataFrame for submission\nsubmission_df = pd.DataFrame({\n    'Sr No.': all_ids,\n    'Sentiment': sentiments\n})\n\n# Save the DataFrame to a CSV file\nsubmission_df.to_csv(\"submission.csv\", index=False)\n\nprint(\"Submission file 'submission.csv' created successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:38:21.410088Z","iopub.execute_input":"2024-11-11T16:38:21.411185Z","iopub.status.idle":"2024-11-11T16:38:21.425308Z","shell.execute_reply.started":"2024-11-11T16:38:21.411130Z","shell.execute_reply":"2024-11-11T16:38:21.423917Z"}},"outputs":[],"execution_count":null}]}